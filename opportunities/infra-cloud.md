# â˜ï¸ Infra/Cloud

Deploy, monitoring, PaaS self-hosted.

### [coollabsio/coolify](https://github.com/coollabsio/coolify) â­ 50.1k | ğŸ¯ğŸ’¸ğŸš€
**Problema:** Vercel/Netlify ficam caros em produÃ§Ã£o ($20-100/mÃªs por projeto).
**SoluÃ§Ã£o:** PaaS self-hosted: deploy 1-click no seu VPS de $5/mÃªs. DX de Vercel, preÃ§o de VPS.
**Por que Ã© superior:** ğŸ’¸ $5/mÃªs vs $100+ no Vercel. ğŸš€ 50k stars = maior PaaS open-source. ğŸ¯ Todo dev precisa de deploy.

---

### [dokploy/dokploy](https://github.com/dokploy/dokploy) â­ 29.7k | ğŸ¯ğŸ’¸
**Problema:** Mesmo problema do Coolify â€” deploy caro e lock-in em plataformas cloud.
**SoluÃ§Ã£o:** PaaS open-source com Docker: deploy, databases, certificados, tudo em um painel.
**Por que Ã© superior:** ğŸ’¸ GrÃ¡tis vs Heroku/Railway. ğŸ¯ 29k stars, UX moderna. Alternativa mais leve ao Coolify.

---

### [openstatusHQ/openstatus](https://github.com/openstatusHQ/openstatus) â­ 8.3k | ğŸ¯ğŸ’¸
**Problema:** Pingdom ($15/mÃªs) + Statuspage ($29/mÃªs) = $44/mÃªs sÃ³ pra monitorar uptime.
**SoluÃ§Ã£o:** Monitoring + status page unificados, open-source.
**Por que Ã© superior:** ğŸ’¸ 2 ferramentas pagas substituÃ­das por 1 grÃ¡tis. ğŸ¯ Todo SaaS precisa de status page.

---

### [skypilot-org/skypilot](https://github.com/skypilot-org/skypilot) â­ 9.4k | ğŸ’¸âš¡
**Problema:** Workloads de AI ficam presos num cloud provider caro (GPU on-demand Ã© proibitivo).
**SoluÃ§Ã£o:** Orquestrador multi-cloud: roda AI workloads no provider mais barato automaticamente.
**Por que Ã© superior:** ğŸ’¸ Encontra GPU 3-5x mais barata entre providers. âš¡ Setup automÃ¡tico, sem lock-in.

---

### [alibaba/higress](https://github.com/alibaba/higress) â­ 7.4k | âš¡ğŸ“ˆ
**Problema:** API gateways tradicionais nÃ£o entendem trÃ¡fego de LLMs (tokens, rate limiting por modelo).
**SoluÃ§Ã£o:** API gateway AI-native: roteamento inteligente de LLMs, load balancing por custo/latÃªncia.
**Por que Ã© superior:** âš¡ Roteamento inteligente reduz latÃªncia. ğŸ“ˆ Escala trÃ¡fego de AI sem config manual.

---


### [ai-dynamo/dynamo](https://github.com/ai-dynamo/dynamo) â­ 6.0k | âš¡ğŸ“ˆğŸš€ğŸ’¸
**Problema:** Servir LLMs em produÃ§Ã£o requer orquestraÃ§Ã£o complexa de GPUs, especialmente para modelos grandes que excedem capacidade de uma GPU. Tensor parallelism cria gargalos de coordenaÃ§Ã£o. Empresas gastam fortunas em infra subotimizada.
**SoluÃ§Ã£o:** NVIDIA Dynamo â€” framework open-source de inferÃªncia distribuÃ­da. Disaggregated prefill & decode, dynamic GPU scheduling, LLM-aware request routing, KV cache offloading. Engine-agnostic (TRT-LLM, vLLM, SGLang). Rust + Python.
**Por que Ã© 5-10x melhor:**
- âš¡ **Velocidade:** Disaggregated serving + NIXL = latÃªncia dramaticamente menor
- ğŸ“ˆ **Volume:** Datacenter-scale, multi-node nativo â€” nÃ£o Ã© wrapper, Ã© framework fundamental
- ğŸš€ **Escala:** De single-GPU para clusters com scheduling dinÃ¢mico
- ğŸ’¸ **Custo:** Open-source vs proprietary serving (Anyscale, Modal pricing). GPU utilization otimizada = menos GPUs necessÃ¡rias
**TAM:** $10B+ (AI inference infrastructure, o mercado mais quente de 2025-2026)
**Modelo de negÃ³cio:** Open-core (NVIDIA vende hardware + consulting), mas terceiros podem construir managed platforms em cima
**EsforÃ§o:** Alto â€” requer expertise em GPU infra e distributed systems
**CombinaÃ§Ãµes:** Dynamo + qualquer modelo open-source = serving de produÃ§Ã£o. Dynamo + deepinfra/together model = competir com OpenAI em custo
