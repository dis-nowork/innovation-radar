# ü§ñ AI Workforce, Video Generation & IoT ‚Äî Fev 2026

## 362. eigent-ai/eigent ‚≠ê 11.9k
**Link:** https://github.com/eigent-ai/eigent
**Categoria:** AI/Workforce Desktop | **License:** Apache-2.0 | **Lang:** TypeScript

**Problema real:** Knowledge workers gastam 40-60% do tempo em tarefas repetitivas (email, data entry, report generation, scheduling). Ferramentas como Cowork (Anthropic) custam $25-50/m√™s/usu√°rio e s√£o cloud-only. PMEs e equipes reguladas precisam de alternativa local.

**O que faz:** Desktop app open-source que permite criar, gerenciar e deployar uma "workforce" de AI agents customizados. Cada agent pode usar MCP tools, executar tasks em paralelo, e ser orquestrado por um humano via chat. Built em cima do CAMEL-AI framework.

**Eixos de inova√ß√£o:**
- üéØ **Problema real:** Automa√ß√£o de tarefas repetitivas sem vendor lock-in
- üí∏ **5-10x menor custo:** $0 self-hosted vs $25-50/user/m√™s (Cowork/Cluely)
- üöÄ **5-10x mais escala:** Multi-agent paralelo, n√£o limitado a 1 chat por vez
- üíé **Qualidade:** SSO, access control, enterprise features que alternativas n√£o t√™m

**TAM:** $15B+ (productivity software + AI assistants)
**Modelo de neg√≥cio:** Open core (community free) + Enterprise (SSO, audit logs, managed hosting)
**Esfor√ßo:** M√©dio ‚Äî j√° tem UI polida, falta integra√ß√µes verticais
**Combina√ß√µes:** + MCP servers (#315 n8n-mcp, #118 mcp-chrome) = workforce que opera em qualquer ferramenta
**Competidores:** Cowork ($25/m√™s), Cluely ($280/m√™s), openwork (#92)

---

## 363. rednote-hilab/dots.ocr ‚≠ê 7.1k
**Link:** https://github.com/rednote-hilab/dots.ocr
**Categoria:** AI/Document Parsing | **License:** MIT | **Lang:** Python

**Problema real:** Document parsing requer pipelines complexos: detector de layout (YOLO) ‚Üí OCR engine ‚Üí table extractor ‚Üí formula parser. Cada component falha de forma diferente, e a integra√ß√£o √© fr√°gil. ABBYY cobra $500k+/ano enterprise. AWS Textract cobra por volume.

**O que faz:** Um √öNICO modelo VLM de 1.7B params que faz layout detection + OCR + table extraction + formula recognition + reading order, tudo com um prompt swap. SOTA no OmniDocBench em EN, ZH e multil√≠ngue. By RedNote (Xiaohongshu ‚Äî 300M+ MAU).

**Eixos de inova√ß√£o:**
- üéØ **Problema real:** Substitui pipelines multi-model por 1 modelo
- üíé **5-10x qualidade:** SOTA em benchmarks ‚Äî supera modelos 10x maiores (Gemini 2.5 Pro compar√°vel em f√≥rmulas)
- ‚ö° **5-10x mais r√°pido:** 1 forward pass vs pipeline de 4-5 models sequenciais
- üí∏ **Custo:** MIT, self-host√°vel, 1.7B roda em GPU consumer

**TAM:** $15B+ (Intelligent Document Processing ‚Äî ABBYY, Kofax, AWS Textract)
**Modelo de neg√≥cio:** API managed, enterprise on-prem, integra√ß√£o em SaaS (contabilidade, legal, healthcare)
**Esfor√ßo:** M√©dio ‚Äî modelo pronto, precisa wrapping (API, batch processing, webhooks)
**Combina√ß√µes:** + Unstract (#94) + CocoIndex (#95) = pipeline doc intelligence completa em 1 stack
**Diferencial vs DeepSeek-OCR (#129):** dots.ocr unifica layout+OCR; DeepSeek foca em compress√£o √≥ptica

---

## 364. boson-ai/higgs-audio ‚≠ê 7.9k
**Link:** https://github.com/boson-ai/higgs-audio
**Categoria:** Voice AI/Foundation Model | **License:** Apache-2.0 | **Lang:** Python

**Problema real:** TTS de alta qualidade com expressividade (emo√ß√µes, ritmo natural, multi-speaker) requer modelos enormes e caros. ElevenLabs cobra $0.15-0.30/1k chars. V√≠deo dubbing profissional custa $50-200/min.

**O que faz:** Foundation model de √°udio treinado em 10M+ horas. V2.5 condensa tudo em 1B params. Gera di√°logos multi-speaker, adapta pros√≥dia automaticamente em narra√ß√£o, faz humming mel√≥dico com voz clonada, e gera speech com m√∫sica de fundo simultaneamente. 75.7% win rate vs GPT-4o-mini-tts em "Emotions" no EmergentTTS-Eval.

**Eixos de inova√ß√£o:**
- üéØ **Problema real:** TTS expressivo √© caro e bloqueado por APIs
- üíé **5-10x qualidade:** Win rate 75% vs GPT-4o-mini-tts; capacidades emergentes (multi-speaker dialogue, music+speech, live translation)
- üöÄ **Escala:** V2.5 1B params = roda em hardware acess√≠vel, deploy at scale

**TAM:** $8B+ (TTS market + localization + content creation)
**Modelo de neg√≥cio:** API managed ($), enterprise license, integra√ß√£o em plataformas de cria√ß√£o de conte√∫do
**Esfor√ßo:** M√©dio ‚Äî requer GPU inferencing, mas Docker pronto
**Combina√ß√µes:** + InfiniteTalk (#368) + OpenCut (#83) = pipeline dubbing automatizado completo

---

## 365. JerryZLiu/Dayflow ‚≠ê 5.7k
**Link:** https://github.com/JerryZLiu/Dayflow
**Categoria:** Produtividade/Time Tracking | **License:** MIT | **Lang:** Swift

**Problema real:** Knowledge workers n√£o sabem onde gastam tempo. RescueTime ($12/m√™s) e Timing ($9/m√™s) s√£o caros, cloud-dependent, e privacy-invasive. Freelancers precisam trackear horas para faturamento mas odeiam timers manuais.

**O que faz:** App macOS nativo que monitora sua tela, usa AI (Gemini/Ollama/Claude) para entender o que voc√™ est√° fazendo, e gera timeline do dia automaticamente. 25MB, <1% CPU, 100MB RAM. 100% local com modelos locais.

**Eixos de inova√ß√£o:**
- üéØ **Problema real:** Time tracking passivo para freelancers/profissionais
- üí∏ **5-10x menor custo:** $0 vs $9-12/m√™s (Timing/RescueTime)
- üíé **Qualidade:** AI summarization d√° contexto, n√£o s√≥ "tempo no Chrome"

**TAM:** $3B+ (time tracking + workforce analytics)
**Modelo de neg√≥cio:** Freemium desktop (gr√°tis local, pago para cloud sync/teams), enterprise (workforce analytics)
**Esfor√ßo:** Baixo ‚Äî j√° funciona como produto, falta cross-platform (Windows/Linux)
**Combina√ß√µes:** + Screenpipe (#86) para capture + Dayflow para summarization = combo letal

---

## 366. francescopace/espectre ‚≠ê 6.2k
**Link:** https://github.com/francescopace/espectre
**Categoria:** IoT/Smart Home | **License:** GPL-3.0 | **Lang:** Python (ESPHome)

**Problema real:** Motion detection em smart homes depende de sensores PIR (limitados, √¢ngulo estreito), c√¢meras (invas√£o de privacidade), ou sensores mmWave ($30-50/unidade). Para eldercare e seguran√ßa residencial, precisa cobrir casa inteira com m√∫ltiplos sensores caros.

**O que faz:** Transforma Wi-Fi em sensor de movimento usando Channel State Information (CSI). Um ESP32 de ‚Ç¨10 detecta movimento analisando perturba√ß√µes nos sinais Wi-Fi. Abordagem 100% matem√°tica (sem ML), integra√ß√£o nativa com Home Assistant via ESPHome. Setup em 10-15 minutos.

**Eixos de inova√ß√£o:**
- üéØ **Problema real:** Home security/eldercare sem c√¢meras
- üí∏ **5-10x menor custo:** ‚Ç¨10 ESP32 vs $30-50/sensor mmWave (ou $200+ c√¢meras)
- ‚ö° **5-10x mais r√°pido setup:** 10-15 min vs horas de instala√ß√£o de c√¢meras
- üöÄ **Escala:** WiFi j√° existe em toda casa ‚Äî cada roteador √© potencial sensor

**TAM:** $5B+ (home security) + $15B+ (eldercare monitoring)
**Modelo de neg√≥cio:** Kit de hardware + firmware, SaaS de monitoramento (eldercare), white-label para builders
**Esfor√ßo:** Baixo ‚Äî produto funciona, precisa packaging comercial
**Combina√ß√µes:** + Home Assistant + ElatoAI (#70) = casa inteligente que detecta movimento por Wi-Fi e conversa por voz

---

## 367. Wan-Video/Wan2.2 ‚≠ê 14.0k
**Link:** https://github.com/Wan-Video/Wan2.2
**Categoria:** AI/Video Generation | **License:** Apache-2.0 | **Lang:** Python

**Problema real:** Gera√ß√£o de v√≠deo com qualidade cinematogr√°fica requer Sora ($200/m√™s), Runway ($76/m√™s), ou Kling (pago). Nenhum modelo open-source combina est√©tica cinematic + 720P@24fps + roda em GPU consumer.

**O que faz:** Modelos MoE de video generation com qualidade cinematogr√°fica (Alibaba/DAMO). Wan2.2 5B model gera 720P@24fps e roda em 4090 consumer. Suporta T2V, I2V, animate (character animation), speech-to-video, e v√≠deos com m√∫sica sincronizada. +83% mais dados de treino que v2.1.

**Eixos de inova√ß√£o:**
- üéØ **Problema real:** V√≠deo cinematogr√°fico acess√≠vel para criadores
- üíé **5-10x qualidade:** MoE architecture + aesthetic data labeling = TOP entre modelos open E closed
- üöÄ **Escala:** 5B model em 4090 = democratiza gera√ß√£o de v√≠deo pro-level

**TAM:** $10B+ (video creation tools + advertising + entertainment)
**Modelo de neg√≥cio:** API service, SaaS de cria√ß√£o de v√≠deo, enterprise licensing
**Esfor√ßo:** Alto ‚Äî requer GPU heavy, mas inference pipeline existe
**Combina√ß√µes:** + InfiniteTalk (#368) para dubbing + Higgs-Audio (#364) para voz = studio de produ√ß√£o AI completo

---

## 368. MeiGen-AI/InfiniteTalk ‚≠ê 4.7k
**Link:** https://github.com/MeiGen-AI/InfiniteTalk
**Categoria:** AI/Talking Video | **License:** Apache-2.0 | **Lang:** Python

**Problema real:** Dubbing de v√≠deo √© um processo manual que custa $50-200/minuto. Talking head videos para courses/marketing s√£o caros (Synthesia $22-67/m√™s com limita√ß√µes). V√≠deos de dura√ß√£o longa perdem sincroniza√ß√£o labial com m√©todos existentes.

**O que faz:** Modelo de gera√ß√£o de talking videos de dura√ß√£o ILIMITADA com lip-sync perfeito. Suporta: √°udio‚Üív√≠deo (dubbing), imagem‚Üív√≠deo (foto vira talking head), continua√ß√£o de v√≠deo (estende cenas). By Meituan (food delivery giant ‚Äî massive video needs).

**Eixos de inova√ß√£o:**
- üéØ **Problema real:** Dubbing e talking head videos s√£o caros/limitados
- üíé **Qualidade:** Unlimited-length = sem degrada√ß√£o em v√≠deos longos (outros modelos falham >30s)
- ‚ö° **Velocidade:** Audio-driven = resultado r√°pido sem itera√ß√£o manual
- üöÄ **Escala:** Uma foto + √°udio = v√≠deo; um v√≠deo + novo √°udio = dubbing em qualquer idioma

**TAM:** $8B+ (video dubbing + localization + e-learning + marketing)
**Modelo de neg√≥cio:** API de dubbing pay-per-minute, SaaS para criadores, enterprise licensing
**Esfor√ßo:** M√©dio ‚Äî modelo pronto, precisa wrapping e UX
**Combina√ß√µes:** + Higgs-Audio (#364) para TTS + dots.ocr (#363) para transcri√ß√£o de legendas = pipeline de localiza√ß√£o de v√≠deo completa e automatizada
